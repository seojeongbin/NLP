{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keybert_kor.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 기업은행 관련 기사 (콜센터) 관련 내용에서 bert를 이용하여 키워드 추출하는 예제"
      ],
      "metadata": {
        "id": "I45PLcNyajgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riKsp5N93Ld4",
        "outputId": "d3c65f3c-51e5-4a26-e1af-13f7ce3a64fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOmAceu1kWr9",
        "outputId": "de90494c-e284-43da-c149-76d0f15d38bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.19.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "특정 문서의 주요 정보를 이해하고자 할 때 키워드 추출을 통해서 입력 텍스트와 가장 관련이 있는 단어와 구문을 추출할 수 있습니다. 'Rake'나 'YAKE!'와 같은 키워드를 추출하는 데 사용할 수 있는 패키지가 이미 존재합니다. 그러나 이러한 모델은 일반적으로 텍스트의 통계적 특성에 기반하여 작동하며 의미적인 유사성에 대해서는 고려하지 않습니다. 의미적 유사성을 고려하기 위해서 여기서는 SBERT 임베딩을 활용하여 사용하기 쉬운 키워드 추출 알고리즘인 KeyBERT를 사용합니다."
      ],
      "metadata": {
        "id": "90aIbIiLrqz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 기본 KeyBERT"
      ],
      "metadata": {
        "id": "TaxveAOApIru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "d1eODWZ6kw2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"\"\"\n",
        "IBK기업은행이 한국능률협회컨설팅(KMAC)에서 주관하는 '2022년 한국산업의 서비스 품질지수(KSQI)' 콜센터 부문에서 16년 연속 '한국의 우수콜센터'로 선정됐다.\n",
        "\n",
        "기업은행은 '고객중심'의 고객센터를 실현하기 위해 △수신여건 개선 △AI(인공지능)기술 활용 △상담품질 개선 △상담직원 케어 등 다양한 노력을 기울였고 그 결과 16년 연속 우수콜센터 선정이라는 명예를 달성할 수 있었다.\n",
        "\n",
        "먼저 코로나19 팬데믹으로 소상공인 지원대출 문의 증가, 비대면 전용상품 출시 증가 등 인입콜이 증가함에 따라 '수신여건' 고도화에 집중했다. 특정 업무로 집중되는 콜을 분산하기 위해 해당 서비스코드 전면 배치, 안내멘트 송출 등 ARS 시스템을 개선했고, 특정 이슈 발생 시 긴급 투입이 가능한 신속 대응팀도 신설·운영했다.\n",
        "\n",
        "업무 생산성을 높이기 위해 기업뱅킹·퇴직연금·소상공인 대출 등 상담직원 간 순환근무 활성화로 전문적이고 복합적인 기업업무 문의에 대해 원스톱(ONE-STOP) 서비스를 실시했다.\n",
        "\n",
        "또한 언택트 시대 도래로 비대면 채널 이용고객이 증가함에 따라 AI분석기법인 텍스트 마이닝과 딥러닝 기법을 활용해 고객센터로 유입되는 다양한 고객의 소리를 면밀히 분석했다. 이러한 분석 결과를 직원들에게 전파하고 실제 상담과정에 반영해 고객과의 공감과 소통에 역점을 두었다.\n",
        "\n",
        "기업은행 콜센터는 상담 품질 개선에도 아낌없는 투자와 지원을 했다. 특히 다양하고 세분화된 고객 문의에 응대하기 위해 상담인력에 대한 단계적이고 체계적인 교육, 그리고 역량 강화에 힘썼다. 또한 다양한 비대면 교육 채널을 통한 전문지식 함양과 업무역량 향상에 많은 노력을 기울였다.\n",
        "\n",
        "기업은행 관계자는 \"앞으로도 기업은행 고객센터만의 '더 쉽고, 더 빠르게, 더 편리하게' 특화된 서비스를 제공하겠다\"고 말했다.\n",
        "      \"\"\""
      ],
      "metadata": {
        "id": "YjY9MmrPkPbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "tokenized_doc = okt.pos(doc)\n",
        "tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
        "\n",
        "print('품사 태깅 10개만 출력 :',tokenized_doc[:10])\n",
        "print('명사 추출 :',tokenized_nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNBysM0d3g4G",
        "outputId": "30569bdd-a167-46f8-cd5e-3c89576af389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태깅 10개만 출력 : [('\\n', 'Foreign'), ('IBK', 'Alpha'), ('기업은행', 'Noun'), ('이', 'Josa'), ('한국', 'Noun'), ('능률', 'Noun'), ('협회', 'Noun'), ('컨설팅', 'Noun'), ('(', 'Punctuation'), ('KMAC', 'Alpha')]\n",
            "명사 추출 : 기업은행 한국 능률 협회 컨설팅 주관 한국 산업 서비스 품질 지수 콜센터 부문 연속 한국 우수 콜센터 로 선정 기업은행 고객 중심 의 고객 센터 실현 위해 수신 여건 개선 인공 지능 기술 활용 상담 품질 개선 상담 직원 케어 등 노력 그 결과 연속 우수 콜센터 선정 명예 달성 수 먼저 코로나 팬데믹 소상 공인 지원 대출 문의 증가 비대 전용 상품 출시 증가 등 인입콜 증가 함 수신 여건 고도화 집중 특정 업무 집중 콜 분산 위해 해당 서비스 코드 전면 배치 안내 멘트 송출 등 시스템 개선 특정 이슈 발생 시 긴급 투입 신속 대응 팀 신설 운영 업무 생산 위해 기업 뱅킹 퇴직 연금 소상 공인 대출 등 상담 직원 간 순환 근무 활성화 전문 복합 기업 업무 문의 대해 원스톱 서비스 실시 또한 언택트 시대 도래 비대 채널 이용 고객 증가 함 분석 법인 텍스트 마 이닝 딥 러닝 기법 활용 고객 센터 유입 고객 소리 분석 분석 결과 직원 전파 실제 상담 과정 반영 고객 공감 소통 역점 기업은행 콜센터 상담 품질 개선 투자 지원 세분 고객 문의 응대 위해 상담 인력 대한 단계 체계 교육 역량 강화 또한 비대 교육 채널 통한 전문 지식 함양 업무 역량 향상 노력 기업은행 관계자 앞 기업은행 고객 센터 더 더 더 특화된 서비스 제공 고 말\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서는 사이킷런의 CountVectorizer를 사용하여 단어를 추출합니다.  CountVectorizer를 사용하는 이유는 n_gram_range의 인자를 사용하면 단쉽게 n-gram을 추출할 수 있기 때문입니다. 예를 들어, (2, 3)으로 설정하면 결과 후보는 2개의 단어를 한 묶음으로 간주하는 bigram과 3개의 단어를 한 묶음으로 간주하는 trigram을 추출합니다."
      ],
      "metadata": {
        "id": "Upr1ps4qswAh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0SC908jkEfz",
        "outputId": "7e65fe12-68cd-4cf0-e285-d06822b46ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trigram 개수 : 361\n",
            "trigram 다섯개만 출력 : ['강화 또한' '강화 또한 비대' '개선 상담' '개선 상담 직원' '개선 인공']\n"
          ]
        }
      ],
      "source": [
        "n_gram_range = (2, 3)\n",
        "\n",
        "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
        "candidates = count.get_feature_names_out()\n",
        "\n",
        "print('trigram 개수 :',len(candidates))\n",
        "print('trigram 다섯개만 출력 :',candidates[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음으로 이제 문서와 문서로부터 추출한 키워드들을 SBERT를 통해서 수치화 할 차례입니다. 한국어를 포함하고 있는 다국어 SBERT를 로드합니다."
      ],
      "metadata": {
        "id": "h7Dqv6jwv94E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')"
      ],
      "metadata": {
        "id": "zGZKkdNdkUF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embedding = model.encode([doc])\n",
        "candidate_embeddings = model.encode(candidates)"
      ],
      "metadata": {
        "id": "UTrVhm8349Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 문서와 가장 유사한 키워드들을 추출합니다. 여기서는 문서와 가장 유사한 키워드들은 문서를 대표하기 위한 좋은 키워드라고 가정합니다. 상위 5개의 키워드를 출력합니다."
      ],
      "metadata": {
        "id": "E9KGcs62p5OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 5\n",
        "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
      ],
      "metadata": {
        "id": "8l3aJzWTkpgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7PkJYUAmOWN",
        "outputId": "7e07fbe2-1ec4-422d-ad87-6800e0c27250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['콜센터 선정 기업은행', '개선 투자 지원', '연속 우수 콜센터', '기업은행 한국 능률', '향상 노력 기업은행']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5개의 키워드가 출력되는데, 이들의 의미가 좀 비슷해보입니다. 비슷한 의미의 키워드들이 리턴되는 데는 이유가 있습니다. 당연히 이 키워드들이 문서를 가장 잘 나타내고 있기 때문입니다. 만약, 지금 출력한 것보다는 좀 더 다양한 의미의 키워드들이 출력된다면 이들을 그룹으로 본다는 관점에서는 어쩌면 해당 키워드들이 문서를 잘 나타낼 가능성이 적을 수도 있습니다. 따라서 키워드들을 다양하게 출력하고 싶다면 키워드 선정의 정확성과 키워드들의 다양성 사이의 미묘한 균형이 필요합니다."
      ],
      "metadata": {
        "id": "m4HPcJOxmMxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서는 다양한 키워드들을 얻기 위해서 두 가지 알고리즘을 사용합니다."
      ],
      "metadata": {
        "id": "_HgqGxytwtFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Max Sum Similarity  \n",
        "* Maximal Marginal Relevance"
      ],
      "metadata": {
        "id": "AupvVbihwy4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Max Sum Similarity"
      ],
      "metadata": {
        "id": "IoXd7eWFlzVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 쌍 사이의 최대 합 거리는 데이터 쌍 간의 거리가 최대화되는 데이터 쌍으로 정의됩니다. 여기서의 의도는 후보 간의 유사성을 최소화하면서 문서와의 후보 유사성을 극대화하고자 하는 것입니다."
      ],
      "metadata": {
        "id": "hOMeOmkLl2oJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
        "    # 문서와 각 키워드들 간의 유사도\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "\n",
        "    # 각 키워드들 간의 유사도\n",
        "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
        "                                            candidate_embeddings)\n",
        "\n",
        "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n",
        "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
        "    words_vals = [candidates[index] for index in words_idx]\n",
        "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
        "\n",
        "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
        "    min_sim = np.inf\n",
        "    candidate = None\n",
        "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
        "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
        "        if sim < min_sim:\n",
        "            candidate = combination\n",
        "            min_sim = sim\n",
        "\n",
        "    return [words_vals[idx] for idx in candidate]"
      ],
      "metadata": {
        "id": "7I4MpmMhk2JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이를 위해 상위 10개의 키워드를 선택하고 이 10개 중에서 서로 가장 유사성이 낮은 5개를 선택합니다."
      ],
      "metadata": {
        "id": "hSHRT4GyrU6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "낮은 nr_candidates를 설정하면 결과는 출력된 키워드 5개는 기존의 코사인 유사도만 사용한 것과 매우 유사한 것으로 보입니다."
      ],
      "metadata": {
        "id": "cVEwuvdtoNbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8HbW4xvnw6S",
        "outputId": "a87f0b30-e576-4d40-a064-c82cec1015b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['한국 우수 콜센터', '투자 지원', '개선 투자', '고객 센터 실현', '기업은행 한국 능률']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그러나 상대적으로 높은 nr_candidates는 더 다양한 키워드 5개를 만듭니다."
      ],
      "metadata": {
        "id": "wyENsqxKoN39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELoogEc9oKyc",
        "outputId": "4f5667a3-d8c3-4267-c6af-25ca08d8994f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['역점 기업은행 콜센터', '상품 출시 증가', '품질 개선 투자', '기업은행 한국', '우수 콜센터 선정']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Maximal Marginal Relevance"
      ],
      "metadata": {
        "id": "q8TJsvU-p1a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과를 다양화하는 마지막 방법은 MMR(Maximum Limit Relegance)입니다. MMR은 텍스트 요약 작업에서 중복을 최소화하고 결과의 다양성을 극대화하기 위해 노력합니다. 참고 할 수 있는 자료로 EmbedRank(https://arxiv.org/pdf/1801.04470.pdf) 라는 키워드 추출 알고리즘은 키워드를 다양화하는 데 사용할 수 있는 MMR을 구현했습니다. 먼저 문서와 가장 유사한 키워드를 선택합니다. 그런 다음 문서와 유사하고 이미 선택된 키워드와 유사하지 않은 새로운 후보를 반복적으로 선택합니다."
      ],
      "metadata": {
        "id": "d6Oc7LwspRCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
        "\n",
        "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
        "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
        "\n",
        "    # 각 키워드들 간의 유사도\n",
        "    word_similarity = cosine_similarity(candidate_embeddings)\n",
        "\n",
        "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
        "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
        "    # keywords_idx = [2]\n",
        "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "\n",
        "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
        "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
        "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
        "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "\n",
        "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
        "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
        "    for _ in range(top_n - 1):\n",
        "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "        # MMR을 계산\n",
        "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
        "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "        # keywords & candidates를 업데이트\n",
        "        keywords_idx.append(mmr_idx)\n",
        "        candidates_idx.remove(mmr_idx)\n",
        "\n",
        "    return [words[idx] for idx in keywords_idx]"
      ],
      "metadata": {
        "id": "Ft7LAjDXonB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 우리가 상대적으로 낮은 diversity 값을 설정한다면, 결과는 기존의 코사인 유사도만 사용한 것과 매우 유사한 것으로 보입니다."
      ],
      "metadata": {
        "id": "jPExUo11o213"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfxryZSOopAy",
        "outputId": "37628bf5-70fd-413a-a604-151d341c5935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['향상 노력 기업은행', '연속 우수 콜센터', '기업은행 한국 능률', '고객 센터 실현', '개선 투자 지원']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그러나 상대적으로 높은 diversity값은 다양한 키워드 5개를 만들어냅니다."
      ],
      "metadata": {
        "id": "gL_gQ3Rlo57T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlMapb1covnK",
        "outputId": "5d10a11c-3410-4933-82e3-d45b5521bcd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['향상 노력 기업은행', '연속 한국 우수', '인공 지능', '결과 직원 전파', '공감 소통 역점']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}